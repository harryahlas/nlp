{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WordVectorsSneap.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-O7UP3Tox_f4","colab_type":"text"},"source":["https://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n"]},{"cell_type":"code","metadata":{"id":"PIhL4jwNzT_N","colab_type":"code","colab":{}},"source":["# import numpy as np\n","# import keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"biIRQYEU0A-D","colab_type":"code","colab":{}},"source":["url = 'http://mattmahoney.net/dc/'\n","filename = maybe_download('text8.zip', url, 31344016)\n","root_path = \"C:\\\\Users\\Andy\\PycharmProjects\\\\adventures-in-ml-code\\\\\"\n","if not os.path.exists((root_path + filename).strip('.zip')):\n","    zipfile.ZipFile(root_path+filename).extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HV8wz63J3bT","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nISiMesKLejv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"outputId":"fa173918-e21c-4427-88c6-c16725cce7de","executionInfo":{"status":"ok","timestamp":1568085531752,"user_tz":420,"elapsed":17271,"user":{"displayName":"harry ahlas","photoUrl":"","userId":"11515999622722943475"}}},"source":["!pip install regex"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n","\u001b[K     |████████████████████████████████| 655kB 3.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: regex\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609226 sha256=641a41cd0e91e4dd2f4986d219882d8727019e30fddf4c29b396bf8f6ef74e1f\n","  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n","Successfully built regex\n","Installing collected packages: regex\n","Successfully installed regex-2019.8.19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0AajDxssJS7w","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import regex as re\n","import matplotlib.pyplot as plt\n","plt.style.use(\"ggplot\")\n","\n","import regex"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASTYsrATKL5l","colab_type":"code","colab":{}},"source":["df_pages_all = pd.read_pickle(\"/content/gdrive/My Drive/df_pages_all_backup.pkl\")\n","#Filter out nulls\n","df = df_pages_all[df_pages_all.initial_message_text.notnull() == True]\n","\n","from sklearn.model_selection import train_test_split\n","\n","train, test = train_test_split(df, test_size=0.2)\n","\n","# docs = train.initial_message_text.to_list()\n","# labels = train.section_number.to_list()\n","\n","# docs_test = test.initial_message_text.to_list()\n","# labels_test = test.section_number.to_list()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jtXmk9sJJgjJ","colab_type":"code","colab":{}},"source":["# path = 'data/'\n","\n","# TRAIN_DATA_FILE = path + 'train.csv'\n","# TEST_DATA_FILE = path + 'test.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-d42hR2FK8TC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"outputId":"5038e29b-b033-4f81-92ca-922ce67d4bd6","executionInfo":{"status":"ok","timestamp":1568085700270,"user_tz":420,"elapsed":4359,"user":{"displayName":"harry ahlas","photoUrl":"","userId":"11515999622722943475"}}},"source":["print('Processing text dataset')\n","from nltk.tokenize import WordPunctTokenizer\n","from collections import Counter\n","from string import punctuation, ascii_lowercase\n","#import regex as re\n","from tqdm import tqdm\n","\n","# replace urls\n","re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n","                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n","                    re.MULTILINE|re.UNICODE)\n","# replace ips\n","re_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n","\n","# setup tokenizer\n","tokenizer = WordPunctTokenizer()\n","\n","vocab = Counter()\n","\n","def text_to_wordlist(text, lower=False):\n","    # replace URLs\n","    text = re_url.sub(\"URL\", text)\n","    \n","    # replace IPs\n","    text = re_ip.sub(\"IPADDRESS\", text)\n","    \n","    # Tokenize\n","    text = tokenizer.tokenize(text)\n","    \n","    # optional: lower case\n","    if lower:\n","        text = [t.lower() for t in text]\n","    \n","    # Return a list of words\n","    vocab.update(text)\n","    return text\n","\n","def process_comments(list_sentences, lower=False):\n","    comments = []\n","    for text in tqdm(list_sentences):\n","        txt = text_to_wordlist(text, lower=lower)\n","        comments.append(txt)\n","    return comments\n","\n","\n","# list_sentences_train = list(train_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n","# list_sentences_test = list(test_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n","\n","list_sentences_train = list(train[\"initial_message_text\"].fillna(\"NAN_WORD\").values)\n","list_sentences_test = list(test[\"initial_message_text\"].fillna(\"NAN_WORD\").values)\n","\n","comments = process_comments(list_sentences_train + list_sentences_test, lower=True)\n","\n","print(\"The vocabulary contains {} unique tokens\".format(len(vocab)))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["  2%|▏         | 1012/40558 [00:00<00:04, 9706.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Processing text dataset\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 40558/40558 [00:03<00:00, 11385.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["The vocabulary contains 78292 unique tokens\n"],"name":"stdout"}]}]}